{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section can be skipped if these libraries are already installed\n",
    "# !pip install textdescriptives\n",
    "# !pip install contractions\n",
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emicatx/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "import textdescriptives as td\n",
    "from nltk.stem import PorterStemmer\n",
    "import contractions # TODO: apply contractions\n",
    "import stanza\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# TODO: MAKE THIS LESS REDUNDANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***NEED TO EXPLAIN THIS (LOADING FILES)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Dad, you're on TV again !” I heard Eric's voi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Tyler entered the ward, his daughter Vale...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>His body was failing. He had taken care of it ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I saw the button. It was simple, red, no words...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>The human body is an amazing thing. It is able...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>Once upon a time, a young woman broke her vase...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>Once upon a time, two beautiful diamonds and a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Lora and her team had been searching for the l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>A microbiologist works in a lab, analyzing the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Coherence\n",
       "0     3,000 years have I been fighting. Every mornin...          3\n",
       "1     “Dad, you're on TV again !” I heard Eric's voi...          4\n",
       "2     When Tyler entered the ward, his daughter Vale...          4\n",
       "3     His body was failing. He had taken care of it ...          4\n",
       "4     I saw the button. It was simple, red, no words...          5\n",
       "...                                                 ...        ...\n",
       "1534  The human body is an amazing thing. It is able...          3\n",
       "1535  Once upon a time, a young woman broke her vase...          4\n",
       "1536  Once upon a time, two beautiful diamonds and a...          3\n",
       "1537  Lora and her team had been searching for the l...          5\n",
       "1538  A microbiologist works in a lab, analyzing the...          5\n",
       "\n",
       "[1539 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_transcripts = pd.read_csv('processed_data/transcripts_spacy_formatted.csv').drop(['Unnamed: 0'],axis = 1)\n",
    "display(processed_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'- ([ REDACTED ] The guy who blazed the trail walked up to Samantha at the door with a massive harpoon in his hand\", ' Samantha knelt down beside him and he quickly told her he could not let the rabbit die, a wizard of pure magicks could bring such incredible firepower to a small world', \" '- [ BLANKED ] /u/davidvoracious entered his shadow home\", \" 'There was once a country'that rose before history actually began\", \" 'where he saw the once dominant empire crumble into dust over 25th century shards of world upon world destroyed by his hammer over 2500 years ago\", \" 'He is no longer the ruler of his empire though, his other great trump card has now returned to reclaim dominion\", \" '\", ' Beyond the edge of the earth there was a new territory, there still is', \" '- ( REDACTED ) Several weeks have passed since The Man from Outer Space landed\", ' We are standing on the brink of an interstellar war, we can take action, but first one must be done', ' had master decided to do so so, his lone apprentice would have conquered what there land was left and never allowed us the chance to retake it', \" '- [ REDACTED ] I don't know if you remember the coordinates, but any Imperial native will recognize them, so inform me'- An old officer named Cyborg stood behind Sam and instructed the lad\", \" 'He would need some time to collect the information, but as I possess your\", ' information', \" '\", ' and you are not required to release it', ' Your authority may be revoked at your discretion, please go back to sleep, as master will be coming shortly', \" '- The Blue Man sighed\", \" 'Good boy\", \" '\", ' she said', \" '\", ' she bowed to him and promised just what you demanded of me']\n"
     ]
    }
   ],
   "source": [
    "story_lists = []\n",
    "with open('processed_data/sentences.csv', 'r', newline='') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        story_lists.append(row)\n",
    "print(story_lists[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***NEED TO EXPLAIN THIS (COHESION)***\n",
    "\n",
    "***also, add examples?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohesion (noun overlap)\n",
    "\n",
    "# TODO: consider whether this should be only sentences rather than utterances, as in story_lists\n",
    "\n",
    "noun_pos = ['NOUN', 'PROPN', 'PRON']\n",
    "\n",
    "story_lists_updated = []\n",
    "for i in range(len(story_lists)):\n",
    "    story = story_lists[i]\n",
    "    story_nouns = []\n",
    "    for j in range(len(story)):\n",
    "        sentence_nouns = []\n",
    "        sentence = str(story[j])\n",
    "        doc = nlp(contractions.fix(sentence.lower()))\n",
    "        for token in doc:\n",
    "            if token.pos_ in noun_pos:\n",
    "                sentence_nouns.append(str(token))\n",
    "        story_nouns.append(sentence_nouns)\n",
    "    story_lists_updated.append(story_nouns)\n",
    "\n",
    "all_cohesion_arrays = np.ndarray((len(story_lists),), dtype = np.ndarray)\n",
    "\n",
    "for k in range(len(story_lists_updated)):\n",
    "    string = story_lists_updated[k]\n",
    "    cohesion_array = np.zeros((len(string), len(string)))\n",
    "    for i in range(len(string)):\n",
    "        current_string = string[i]\n",
    "        for j in range(len(string)):\n",
    "            new_string = string[j]\n",
    "            if any(item in new_string for item in current_string):\n",
    "                cohesion_array[i, j] = 1\n",
    "    all_cohesion_arrays[k] = cohesion_array\n",
    "\n",
    "for array in all_cohesion_arrays: # TODO: describe check for symmetry\n",
    "    if not np.array_equal(array, array.T):\n",
    "        raise ValueError(\"Array must be symmetric; check input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-reference cohesion, local\n",
    "\n",
    "# TODO: consider only one sentence long transcripts?\n",
    "\n",
    "for n, array in enumerate(all_cohesion_arrays):\n",
    "    counter = 0\n",
    "    for i in range(array.shape[0] - 1):\n",
    "            counter += array[i, i+1]\n",
    "    try:\n",
    "        coref_local = counter / (array.shape[0] - 1)\n",
    "    except ZeroDivisionError:\n",
    "        coref_local = 0 \n",
    "\n",
    "    feature_set.loc[n, 'coref_local'] = coref_local\n",
    "\n",
    "feature_set = feature_set.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-reference cohesion, global\n",
    "\n",
    "np.seterr('raise')\n",
    "\n",
    "for i, array in enumerate(all_cohesion_arrays):\n",
    "    n = array.shape[0]\n",
    "    counter = (array.sum() - np.diag(array).sum()) / 2\n",
    "    try:\n",
    "        coref_global = counter / (n * (n - 1) / 2)\n",
    "    except Exception as e:\n",
    "        coref_global = 0 \n",
    "\n",
    "    feature_set.loc[i, 'coref_global'] = coref_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***NEED TO EXPLAIN THIS (TEXT DESCRIPTION)***\n",
    "\n",
    "***move this down to the bottom!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emicatx/Library/Python/3.9/lib/python/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    }
   ],
   "source": [
    "text_description = pd.DataFrame()\n",
    "\n",
    "for i in processed_transcripts.index:  \n",
    "    string = processed_transcripts['Text'].loc[i]\n",
    "    doc = nlp(string)\n",
    "    text_description_df = td.extract_metrics(text = string, metrics = [\"readability\", \"dependency_distance\", \"coherence\"], spacy_model = 'en_core_web_sm')\n",
    "    text_description = pd.concat([text_description_df, text_description])\n",
    "\n",
    "text_description_ordered = (text_description[::-1]).reset_index()\n",
    "text_description_ordered = text_description_ordered[['text', 'dependency_distance_mean', 'prop_adjacent_dependency_relation_mean', 'first_order_coherence', 'second_order_coherence']]\n",
    "display(text_description_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_description_ordered.to_csv('text_description_ordered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***NEED TO EXPLAIN THIS (STANZA NP/VP/SYNTAX)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f11d1b5d4f4827bc4b3bdd5f143b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 00:18:18 INFO: Downloaded file to /Users/emicatx/stanza_resources/resources.json\n",
      "2025-12-02 00:18:18 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-02 00:18:19 INFO: File exists: /Users/emicatx/stanza_resources/en/default.zip\n",
      "2025-12-02 00:18:21 INFO: Finished downloading models and saved to /Users/emicatx/stanza_resources\n",
      "2025-12-02 00:18:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b67f7f7fe4046fda4f3b4c318859152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 00:18:21 INFO: Downloaded file to /Users/emicatx/stanza_resources/resources.json\n",
      "2025-12-02 00:18:21 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-02 00:18:22 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2025-12-02 00:18:22 INFO: Using device: cpu\n",
      "2025-12-02 00:18:22 INFO: Loading: tokenize\n",
      "2025-12-02 00:18:22 INFO: Loading: mwt\n",
      "2025-12-02 00:18:22 INFO: Loading: pos\n",
      "2025-12-02 00:18:24 INFO: Loading: constituency\n",
      "2025-12-02 00:18:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "model = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation\n",
    "\n",
    "processed_transcripts_updated = []\n",
    "\n",
    "for i in processed_transcripts.index:\n",
    "    string = processed_transcripts['Text'].loc[i]\n",
    "    doc = model(string)\n",
    "    story_updated = []\n",
    "    for sentence in doc.sentences:\n",
    "        tree = sentence.constituency\n",
    "        string_tree = str(tree).split(' ')\n",
    "        story_updated.append(string_tree)\n",
    "\n",
    "    processed_transcripts_updated.append(story_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_transcripts_updated\n",
    "\n",
    "with open(\"processed_data/parsed_data.csv\", \"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(processed_transcripts_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coref_local</th>\n",
       "      <th>coref_global</th>\n",
       "      <th>n_constituents</th>\n",
       "      <th>n_noun_phrases</th>\n",
       "      <th>n_verb_phrases</th>\n",
       "      <th>n_sub_clauses</th>\n",
       "      <th>n_prep_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>152.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>196.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.096959</td>\n",
       "      <td>599.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.105062</td>\n",
       "      <td>546.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>123.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>153.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>70.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coref_local  coref_global  n_constituents  n_noun_phrases  \\\n",
       "0        0.550000      0.433333           152.0            79.0   \n",
       "1        0.650000      0.638095           196.0            93.0   \n",
       "2        0.148148      0.096959           599.0           296.0   \n",
       "3        0.152941      0.105062           546.0           285.0   \n",
       "4        0.600000      0.545455           123.0            64.0   \n",
       "...           ...           ...             ...             ...   \n",
       "1534     0.461538      0.384615           153.0            70.0   \n",
       "1535     0.600000      0.400000            64.0            27.0   \n",
       "1536     0.166667      0.285714            70.0            33.0   \n",
       "1537     0.500000      0.600000            97.0            49.0   \n",
       "1538     0.750000      0.600000            68.0            33.0   \n",
       "\n",
       "      n_verb_phrases  n_sub_clauses  n_prep_phrases  \n",
       "0               49.0           11.0            13.0  \n",
       "1               68.0           12.0            23.0  \n",
       "2              183.0           41.0            79.0  \n",
       "3              172.0           17.0            72.0  \n",
       "4               40.0           11.0             8.0  \n",
       "...              ...            ...             ...  \n",
       "1534            55.0           14.0            14.0  \n",
       "1535            27.0            5.0             5.0  \n",
       "1536            20.0            5.0            12.0  \n",
       "1537            36.0            3.0             9.0  \n",
       "1538            21.0            6.0             8.0  \n",
       "\n",
       "[1539 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Higher-level constituent counter\n",
    "\n",
    "counter_pos = [\"(NP\", \"(VP\", \"(SBAR\", \"(PP\"]\n",
    "\n",
    "for i, story in enumerate(processed_transcripts_updated):\n",
    "    counter = 0\n",
    "    type_counter = [0, 0, 0, 0]\n",
    "    for sentence in story:\n",
    "        for item in sentence:\n",
    "            if item in counter_pos:\n",
    "                counter += 1\n",
    "                idx = counter_pos.index(item)\n",
    "                type_counter[idx] += 1\n",
    "\n",
    "    feature_set.loc[i, 'n_constituents'] = counter\n",
    "    feature_set.loc[i, 'n_noun_phrases'] = type_counter[0]\n",
    "    feature_set.loc[i, 'n_verb_phrases'] = type_counter[1]\n",
    "    feature_set.loc[i, 'n_sub_clauses'] = type_counter[2]\n",
    "    feature_set.loc[i, 'n_prep_phrases'] = type_counter[3]\n",
    "\n",
    "display(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun and verb phrase parser\n",
    "\n",
    "noun_phrases_corpus = []\n",
    "verb_phrases_corpus = []\n",
    "closing_pattern = r'\\)'\n",
    "\n",
    "phrasal_pos = [\"(NP\", \"(VP\"]\n",
    "\n",
    "for story in processed_transcripts_updated:\n",
    "    noun_phrases_overall = []\n",
    "    verb_phrases_overall = []\n",
    "    for sentence in story:\n",
    "        noun_phrases = []\n",
    "        verb_phrases = []\n",
    "        counter = 0\n",
    "        for i in range(len(sentence)):\n",
    "            item = sentence[i]\n",
    "            if item in phrasal_pos:\n",
    "                open_counter = 1\n",
    "                for j in range(i + 1, len(sentence)):\n",
    "                    first_element = sentence[j][0]\n",
    "                    match = re.findall(closing_pattern, sentence[j])\n",
    "                    closed = len(match)\n",
    "                    if first_element == '(':\n",
    "                        open_counter += 1\n",
    "                    open_counter -= closed\n",
    "                    if open_counter <= 0:\n",
    "                        break\n",
    "                phrase = [item for item in sentence[i:j + 1] if item[-1] == ')']\n",
    "                if item == counter_pos[0]:\n",
    "                    noun_phrases.append(phrase)\n",
    "                else:\n",
    "                    verb_phrases.append(phrase)\n",
    "\n",
    "        noun_phrases_overall.append(noun_phrases)\n",
    "        verb_phrases_overall.append(verb_phrases)\n",
    "\n",
    "    noun_phrases_corpus.append(noun_phrases_overall)\n",
    "    verb_phrases_corpus.append(verb_phrases_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean phrase lengths\n",
    "\n",
    "def phrase_length(phrase_corpus: list, phrase_lengths: list) -> list:\n",
    "    \n",
    "    # TODO: type hints + docstrings\n",
    "\n",
    "    for story in phrase_corpus:\n",
    "        length_list = []\n",
    "        for sentence in story:\n",
    "            for phrase in sentence:\n",
    "                length = len(phrase)\n",
    "                length_list.append(length)\n",
    "\n",
    "        length_array = np.array(length_list)\n",
    "        mean = length_array.mean()\n",
    "        phrase_lengths.append(float(mean))\n",
    "\n",
    "    return phrase_lengths\n",
    "\n",
    "\n",
    "mean_np_length = []\n",
    "mean_np_length = phrase_length(noun_phrases_corpus, mean_np_length)\n",
    "\n",
    "mean_vp_length = []\n",
    "mean_vp_length = phrase_length(verb_phrases_corpus, mean_vp_length)\n",
    "\n",
    "feature_set['mean_np_length'] = mean_np_length\n",
    "feature_set['mean_vp_length'] = mean_vp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coref_local</th>\n",
       "      <th>coref_global</th>\n",
       "      <th>n_constituents</th>\n",
       "      <th>n_noun_phrases</th>\n",
       "      <th>n_verb_phrases</th>\n",
       "      <th>n_sub_clauses</th>\n",
       "      <th>n_prep_phrases</th>\n",
       "      <th>mean_np_length</th>\n",
       "      <th>mean_vp_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>152.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.354430</td>\n",
       "      <td>4.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>196.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.516129</td>\n",
       "      <td>7.161765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.096959</td>\n",
       "      <td>599.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.094595</td>\n",
       "      <td>7.650273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.105062</td>\n",
       "      <td>546.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3.010526</td>\n",
       "      <td>6.168605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>123.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.484375</td>\n",
       "      <td>6.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>153.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.871429</td>\n",
       "      <td>9.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.703704</td>\n",
       "      <td>6.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>70.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.424242</td>\n",
       "      <td>7.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.204082</td>\n",
       "      <td>5.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.848485</td>\n",
       "      <td>10.952381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coref_local  coref_global  n_constituents  n_noun_phrases  \\\n",
       "0        0.550000      0.433333           152.0            79.0   \n",
       "1        0.650000      0.638095           196.0            93.0   \n",
       "2        0.148148      0.096959           599.0           296.0   \n",
       "3        0.152941      0.105062           546.0           285.0   \n",
       "4        0.600000      0.545455           123.0            64.0   \n",
       "...           ...           ...             ...             ...   \n",
       "1534     0.461538      0.384615           153.0            70.0   \n",
       "1535     0.600000      0.400000            64.0            27.0   \n",
       "1536     0.166667      0.285714            70.0            33.0   \n",
       "1537     0.500000      0.600000            97.0            49.0   \n",
       "1538     0.750000      0.600000            68.0            33.0   \n",
       "\n",
       "      n_verb_phrases  n_sub_clauses  n_prep_phrases  mean_np_length  \\\n",
       "0               49.0           11.0            13.0        2.354430   \n",
       "1               68.0           12.0            23.0        2.516129   \n",
       "2              183.0           41.0            79.0        3.094595   \n",
       "3              172.0           17.0            72.0        3.010526   \n",
       "4               40.0           11.0             8.0        2.484375   \n",
       "...              ...            ...             ...             ...   \n",
       "1534            55.0           14.0            14.0        2.871429   \n",
       "1535            27.0            5.0             5.0        1.703704   \n",
       "1536            20.0            5.0            12.0        2.424242   \n",
       "1537            36.0            3.0             9.0        2.204082   \n",
       "1538            21.0            6.0             8.0        3.848485   \n",
       "\n",
       "      mean_vp_length  \n",
       "0           4.632653  \n",
       "1           7.161765  \n",
       "2           7.650273  \n",
       "3           6.168605  \n",
       "4           6.775000  \n",
       "...              ...  \n",
       "1534        9.181818  \n",
       "1535        6.481481  \n",
       "1536        7.450000  \n",
       "1537        5.694444  \n",
       "1538       10.952381  \n",
       "\n",
       "[1539 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set.to_csv('processed_data/syntactic_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
