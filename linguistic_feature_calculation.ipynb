{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Feature Calculation\n",
    "This notebook calculates linguistic features from the processed and formatted transcripts. For a full list of relevant features and their definitions, see [this document](README.md). The files output by this notebook contain additional features beyond what is utilized in classification; these files are cleaned to produce a final file with only the included features in [this notebook](feature_processing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I'd like to add further comments in the code to describe the functions, as well as specific examples for each one (when applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be skipped if the needed libraries are already installed\n",
    "# !pip install transformers\n",
    "# !pip install minicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"NotOpenSSLWarning\")\n",
    "warnings.filterwarnings(\"ignore\", message = r\"\\[W007\\].*\", category = UserWarning)\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from minicons import scorer\n",
    "import csv\n",
    "import textdescriptives as td\n",
    "import contractions\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "This section loads the narrative data necessary for the remainder of the script, including the text transcripts in a variety of formats (cleaned with punctuation, cleaned without punctuation, or separated by utterance). Additionally, it loads the language models and datasets for lexical feature analysis (SpaCy, word2vec, and minicons GPT2 surprisal scorer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the processed transcripts (cleaned and formatted, but retains original punctuation for SpaCy tokenization)\n",
    "transcripts_punc = pd.read_csv('processed_data/transcripts_spacy_formatted.csv')\n",
    "\n",
    "# Creating a linguistic feature DataFrame that includes coherence scores\n",
    "linguistic_features = transcripts_punc[['Coherence']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the transcripts with no punctuation\n",
    "transcripts_no_punc = pd.read_csv('processed_data/transcripts_no_punc_formatted.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the utterance-based transcriptions into a list\n",
    "story_lists = []\n",
    "with open('processed_data/sentences.csv', 'r', newline='') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        story_lists.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the SpaCy language processing model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word2vec model\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "sem_sim_model = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model for surprisal\n",
    "surprisal_model = scorer.IncrementalLMScorer('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0977f375e824580afc1bc399a7f33c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 19:09:28 INFO: Downloaded file to /Users/emicatx/stanza_resources/resources.json\n",
      "2025-12-04 19:09:28 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-04 19:09:29 INFO: File exists: /Users/emicatx/stanza_resources/en/default.zip\n",
      "2025-12-04 19:09:32 INFO: Finished downloading models and saved to /Users/emicatx/stanza_resources\n",
      "2025-12-04 19:09:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c466aa1e1e4f449def61d38efcaa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 19:09:32 INFO: Downloaded file to /Users/emicatx/stanza_resources/resources.json\n",
      "2025-12-04 19:09:32 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-04 19:09:33 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2025-12-04 19:09:33 INFO: Using device: cpu\n",
      "2025-12-04 19:09:33 INFO: Loading: tokenize\n",
      "2025-12-04 19:09:33 INFO: Loading: mwt\n",
      "2025-12-04 19:09:33 INFO: Loading: pos\n",
      "2025-12-04 19:09:34 INFO: Loading: constituency\n",
      "2025-12-04 19:09:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Loading the stanza model\n",
    "stanza.download('en')\n",
    "stanza_model = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Calculations\n",
    "This section contains functions and code to calculate each linguistic feature. It uses SpaCy tokenization, Stanza parsing, and the textdescriptives library to determine lexical, syntactic, and macrolinguistic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech_features(input_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the proportion of part-of-speech tags, open-closed class ratio, and propositional density\n",
    "        in a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for part-of-speech proportions, open-closed class ratio, and \n",
    "            propositional density\n",
    "    \"\"\"\n",
    "\n",
    "    pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB', 'X']\n",
    "    open_class_tags = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "    closed_class_tags = ['ADP', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NUM', 'PART', 'PRON', 'SCONJ']\n",
    "    propositional_density_tags = ['ADJ', 'ADV', 'ADP', 'CCONJ', 'SCONJ', 'VERB']\n",
    "\n",
    "    for i in feature_df.index:  \n",
    "\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        doc = nlp(string)\n",
    "        testing = []\n",
    "        open_class = 0\n",
    "        closed_class = 0\n",
    "        prop_density = 0\n",
    "\n",
    "        for token in doc:\n",
    "            testing.append(token.pos_)\n",
    "            if token.pos_ in open_class_tags:\n",
    "                open_class += 1\n",
    "            elif token.pos_ in closed_class_tags:\n",
    "                closed_class += 1\n",
    "            if token.pos_ in propositional_density_tags:\n",
    "                prop_density += 1\n",
    "        \n",
    "        for _, pos in enumerate(pos_tags):\n",
    "            try:\n",
    "                feature_df.loc[i, f'POS:{pos}'] = testing.count(pos)/len(testing)\n",
    "            except ZeroDivisionError:\n",
    "                print(\"No tokens have been identified in the input\")\n",
    "\n",
    "        try:\n",
    "            feature_df.loc[i, 'open_closed_ratio'] = open_class / closed_class\n",
    "        except ZeroDivisionError:\n",
    "            print(f\"Index {i} has no closed class words; check input\")\n",
    "\n",
    "        feature_df.loc[i, 'propositional_density'] = prop_density / len(testing)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the part-of-speech feature function to the transcripts cleaned with punctuation\n",
    "linguistic_features = part_of_speech_features(transcripts_punc ,linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_operators(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\" \n",
    "        Calculates the sum of logical operators (and, or, not, if...then) in an utterance, summed\n",
    "        across all utterances in a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for the number of logical operators\n",
    "    \"\"\"\n",
    "\n",
    "    logical_operator_list = [\n",
    "        (r'^And'),\n",
    "        (r' and[!?\\-,.:;\\'\"\\s]'),\n",
    "        (r' not[!?\\-,.:;\\'\"\\s]'),\n",
    "        (r' or[!?\\-,.:;\\'\"\\s]'),\n",
    "        (r' (if) (.*?)(then)[!?\\-…,.:;\\'\"\\s]')\n",
    "    ]\n",
    "\n",
    "    for i, story in enumerate(input_lists):\n",
    "        logical_operator_number = []\n",
    "        for string in story:\n",
    "            for pattern in logical_operator_list:\n",
    "                match = re.findall(pattern, string, flags = re.IGNORECASE)\n",
    "                n_operators = len(match)\n",
    "                logical_operator_number.append(n_operators)\n",
    "        feature_df.loc[i, 'n_logical_operators'] = np.sum(logical_operator_number)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the logical operators function to the transcripts separated by utterance\n",
    "linguistic_features = logical_operators(story_lists, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_ttr(string: str) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the type-token ratio (number of unique words divided by total number of words)\n",
    "        of a string\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        string: str\n",
    "            String containing text without punctuation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[float, float]\n",
    "            Value of type-token ratio and number of tokens in the string\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError \n",
    "            If type-token ratio is greater than 1\n",
    "        ValueError\n",
    "            If type-token ratio is negative\n",
    "    \"\"\"\n",
    "\n",
    "    string_lower = string.lower()\n",
    "    string_split = string_lower.split(' ')\n",
    "    string_set = set(string_split)\n",
    "    n_tokens = len(string_split)\n",
    "    types = len(string_set)\n",
    "\n",
    "    try:\n",
    "        ttr = types / n_tokens\n",
    "    except ZeroDivisionError:\n",
    "        print(f\"Input string has no tokens (empty string)\")\n",
    "\n",
    "    if ttr > 1:\n",
    "        raise ValueError('Type-token ratio cannot exceed 1')\n",
    "    if ttr < 0:\n",
    "        raise ValueError('Type-token ratio cannot be negative')\n",
    "\n",
    "    return ttr, n_tokens\n",
    "\n",
    "\n",
    "def word_count_features(input_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the number of words and type-token ratio for each transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for type-token ratio and total number of words\n",
    "    \"\"\"\n",
    "\n",
    "    for i in input_df.index:\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        feature_df.loc[i, 'type_token_ratio'], feature_df.loc[i, 'n_words'] = string_ttr(string)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the word count function to the transcripts without punctuations\n",
    "linguistic_features = word_count_features(transcripts_no_punc, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_features(dictionary: dict, feature: str, feature_df: pd.DataFrame, input_df: pd.DataFrame = transcripts_no_punc) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Base function for calculating a series of lexical features (used for word frequency\n",
    "        and semantic diversity)\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        dictionary: dict:\n",
    "            Dictionary containing words and associated pre-defined values, imported from\n",
    "            external sources\n",
    "        feature: str\n",
    "            Feature used to name column of feature DataFrame\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text', defaults to transcripts\n",
    "            with no punctuation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for the relevant feature\n",
    "    \"\"\"\n",
    "\n",
    "    for i in input_df.index:\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        string_split = (string.lower()).split(' ')\n",
    "        values = []\n",
    "\n",
    "        for item in string_split:\n",
    "            value = dictionary.get(item)\n",
    "\n",
    "            if value is not None:\n",
    "                values.append(value)\n",
    "        mean = np.nanmean(np.array(values))\n",
    "\n",
    "        feature_df.loc[i, feature] = mean\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for word frequency based on prior data\n",
    "frequency = pd.read_csv('lexical_feature_data/SUBTLEXusExcel2007.csv')\n",
    "frequency_dict_original = dict(zip(frequency['Word'], frequency['Lg10WF']))\n",
    "frequency_dict = {(word.lower() if isinstance(word, str) else word): value for word, value in frequency_dict_original.items()}\n",
    "\n",
    "# Applying the lexical features function for word frequency to the transcripts without punctuations\n",
    "linguistic_features = lexical_features(frequency_dict, 'log10_freq_mean', linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for semantic diversity based on prior data\n",
    "semantic_diversity = pd.read_csv('lexical_feature_data/13428_2012_278_MOESM1_ESM.csv', dtype = str)\n",
    "semantic_diversity = (semantic_diversity.rename(columns = {'Supplementary Materials: SemD values': 'word', 'Unnamed: 2': 'sem_diversity'})).drop(0)\n",
    "semantic_diversity_dict_original = dict(zip(semantic_diversity['word'], semantic_diversity['sem_diversity']))\n",
    "semantic_diversity_dict = {(word.lower() if isinstance(word, str) else word): (float(value)) for word, value in semantic_diversity_dict_original.items()}\n",
    "\n",
    "# Applying the lexical features function for semantic diversity to the transcripts without punctuations\n",
    "linguistic_features = lexical_features(semantic_diversity_dict, 'semantic_diversity_mean', linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_thematic_similarity(input_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates semantic thematic similarity (cosine similarity between each adjacent word pair\n",
    "        in a transcript)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for semantic thematic similarity\n",
    "    \"\"\"\n",
    "   \n",
    "    for i in input_df.index:  \n",
    "\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        string_split = (string.lower()).split(' ')\n",
    "        sem_sim = []\n",
    "\n",
    "        for j in range(len(string_split) - 1):\n",
    "            try:\n",
    "                sem_sim_value = sem_sim_model.similarity(string_split[j], string_split[j + 1])\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "            sem_sim.append(float(sem_sim_value))\n",
    "        \n",
    "        sem_thematic_distance = np.nanmean(np.array(sem_sim))\n",
    "\n",
    "        feature_df.loc[i, 'semantic_thematic_similarity'] = sem_thematic_distance\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the semantic thematic similarity function to the transcripts without punctuations\n",
    "linguistic_features = semantic_thematic_similarity(transcripts_no_punc, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprisal(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates average surprisal for a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for surprisal\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, story in enumerate(input_lists):\n",
    "        transcript_surprisal = []\n",
    "        \n",
    "        for sentence in story:\n",
    "            sentence_surprisal = surprisal_model.sequence_score(sentence, reduction = lambda x: -x.mean(0).item())\n",
    "            transcript_surprisal.append(sentence_surprisal)\n",
    "        \n",
    "        transcript_surprisal_array = np.array(transcript_surprisal)\n",
    "        surprisal_mean = np.nanmean(transcript_surprisal_array)\n",
    "\n",
    "        feature_df.loc[i, 'surprisal'] = surprisal_mean\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the surprisal function to the transcripts separated by utterance; this cell takes some time to run\n",
    "linguistic_features = surprisal(story_lists, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_cohesion_arrays(input_lists: list) -> list:\n",
    "    \n",
    "    \"\"\" \n",
    "        Calculates noun cohesion/co-reference arrays for a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of arrays, where each array corresponds to a transcript and every element in the \n",
    "            array indicates whether the (i, j) pair of utterances in the transcript shares a \n",
    "            common noun (1) or has no shared elements (0)\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If array is not symmetric, indicating an error in calculation\n",
    "    \"\"\"\n",
    "    \n",
    "    noun_pos = ['NOUN', 'PROPN', 'PRON']\n",
    "\n",
    "    story_lists_updated = []\n",
    "    for story in input_lists:\n",
    "        story_nouns = []\n",
    "        for sentence in story:\n",
    "            sentence_nouns = []\n",
    "            sentence_str = str(sentence)\n",
    "            doc = nlp(contractions.fix(sentence_str.lower()))\n",
    "            for token in doc:\n",
    "                if token.pos_ in noun_pos:\n",
    "                    sentence_nouns.append(str(token))\n",
    "            story_nouns.append(sentence_nouns)\n",
    "        story_lists_updated.append(story_nouns)\n",
    "\n",
    "        \n",
    "    all_cohesion_arrays = np.ndarray((len(input_lists),), dtype = np.ndarray)\n",
    "\n",
    "    for k in range(len(story_lists_updated)):\n",
    "        string = story_lists_updated[k]\n",
    "        cohesion_array = np.zeros((len(string), len(string)))\n",
    "        for i in range(len(string)):\n",
    "            current_string = string[i]\n",
    "            for j in range(len(string)):\n",
    "                new_string = string[j]\n",
    "                if any(item in new_string for item in current_string):\n",
    "                    cohesion_array[i, j] = 1\n",
    "        all_cohesion_arrays[k] = cohesion_array\n",
    "\n",
    "    for array in all_cohesion_arrays:\n",
    "        if not np.array_equal(array, array.T):\n",
    "            raise ValueError(\"Array must be symmetric; check input\")\n",
    "\n",
    "    return all_cohesion_arrays\n",
    "\n",
    "def local_global_cohesion(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates local and global cohesion scores for each transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for local and global cohesion\n",
    "    \"\"\"\n",
    "\n",
    "    input_array = noun_cohesion_arrays(input_lists)\n",
    "\n",
    "    for n, array in enumerate(input_array):\n",
    "        counter = 0\n",
    "        for i in range(array.shape[0] - 1):\n",
    "                counter += array[i, i+1]\n",
    "        try:\n",
    "            coref_local = counter / (array.shape[0] - 1)\n",
    "        except ZeroDivisionError:\n",
    "            coref_local = 0 \n",
    "\n",
    "        feature_df.loc[n, 'coref_local'] = coref_local\n",
    "\n",
    "    feature_df = feature_df.sort_index()\n",
    "\n",
    "    np.seterr('raise')\n",
    "\n",
    "    for i, array in enumerate(input_array):\n",
    "        n = array.shape[0]\n",
    "        counter = (array.sum() - np.diag(array).sum()) / 2\n",
    "        try:\n",
    "            coref_global = counter / (n * (n - 1) / 2)\n",
    "        except Exception as e:\n",
    "            coref_global = 0 \n",
    "\n",
    "        feature_df.loc[i, 'coref_global'] = coref_global\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the local and global cohesion function to the transcripts separated by utterance\n",
    "linguistic_features = local_global_cohesion(story_lists, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(input_df: pd.DataFrame) -> list:\n",
    "\n",
    "    \"\"\"\n",
    "        Uses Stanza to create constituent parse trees for each story\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of lists, where each sublist contains a list with constituent\n",
    "            parsing for each sentence in a transcript\n",
    "    \"\"\"\n",
    "\n",
    "    parsed_transcripts = []\n",
    "\n",
    "    for i in input_df.index:\n",
    "        string = input_df['Text'].loc[i]\n",
    "        doc = stanza_model(string)\n",
    "        story_updated = []\n",
    "        for sentence in doc.sentences:\n",
    "            tree = sentence.constituency\n",
    "            string_tree = str(tree).split(' ')\n",
    "            story_updated.append(string_tree)\n",
    "\n",
    "        parsed_transcripts.append(story_updated)\n",
    "    \n",
    "    return parsed_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the constituency parsing function to the transcripts with punctuation; this cell takes some time to run\n",
    "parsed_transcripts = parser(transcripts_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituent_counter(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the number of sentence-level constituents (noun phrases, verb phrases, \n",
    "        subordinate clauses, and prepositional phrases) in a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for overall number of high-level constituents\n",
    "    \"\"\"\n",
    "\n",
    "    counter_pos = [\"(NP\", \"(VP\", \"(SBAR\", \"(PP\"]\n",
    "\n",
    "    for i, story in enumerate(input_lists):\n",
    "        counter = 0\n",
    "        for sentence in story:\n",
    "            for item in sentence:\n",
    "                if item in counter_pos:\n",
    "                    counter += 1\n",
    "\n",
    "        feature_df.loc[i, 'n_constituents'] = counter\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "def phrase_parser(input_lists: list) -> Tuple[list, list]:\n",
    "    \n",
    "    \"\"\" \n",
    "        Used to isolate noun and verb phrases from parsed and segmented utterances\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[list, list]\n",
    "            List containing each noun phrase for each transcript, and list containing each verb\n",
    "            phrase for each transcript\n",
    "    \"\"\"\n",
    "\n",
    "    noun_phrases_corpus = []\n",
    "    verb_phrases_corpus = []\n",
    "    closing_pattern = r'\\)'\n",
    "\n",
    "    phrasal_pos = [\"(NP\", \"(VP\"]\n",
    "\n",
    "    for story in input_lists:\n",
    "        noun_phrases_overall = []\n",
    "        verb_phrases_overall = []\n",
    "        for sentence in story:\n",
    "            noun_phrases = []\n",
    "            verb_phrases = []\n",
    "            for i in range(len(sentence)):\n",
    "                item = sentence[i]\n",
    "                if item in phrasal_pos:\n",
    "                    open_counter = 1\n",
    "                    for j in range(i + 1, len(sentence)):\n",
    "                        first_element = sentence[j][0]\n",
    "                        match = re.findall(closing_pattern, sentence[j])\n",
    "                        closed = len(match)\n",
    "                        if first_element == '(':\n",
    "                            open_counter += 1\n",
    "                        open_counter -= closed\n",
    "                        if open_counter <= 0:\n",
    "                            break\n",
    "                    phrase = [item for item in sentence[i:j + 1] if item[-1] == ')']\n",
    "                    if item == phrasal_pos[0]:\n",
    "                        noun_phrases.append(phrase)\n",
    "                    else:\n",
    "                        verb_phrases.append(phrase)\n",
    "\n",
    "            noun_phrases_overall.append(noun_phrases)\n",
    "            verb_phrases_overall.append(verb_phrases)\n",
    "\n",
    "        noun_phrases_corpus.append(noun_phrases_overall)\n",
    "        verb_phrases_corpus.append(verb_phrases_overall)\n",
    "\n",
    "    return noun_phrases_corpus, verb_phrases_corpus\n",
    "\n",
    "def phrase_length(phrase_list: list) -> list:\n",
    "    \n",
    "    \"\"\" \n",
    "        Used to calculate the average length of a given phrase type\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        phrase_list: list\n",
    "            Input lists containing each phrase with words as string elements for each\n",
    "            transcript\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List containing the mean length of each phrase for a series of transcripts\n",
    "    \"\"\"\n",
    "\n",
    "    phrase_lengths = []\n",
    "\n",
    "    for story in phrase_list:\n",
    "        length_list = []\n",
    "        for sentence in story:\n",
    "            for phrase in sentence:\n",
    "                length = len(phrase)\n",
    "                length_list.append(length)\n",
    "\n",
    "        length_array = np.array(length_list)\n",
    "        mean = length_array.mean()\n",
    "        phrase_lengths.append(float(mean))\n",
    "\n",
    "    return phrase_lengths\n",
    "\n",
    "def phrase_mean_length(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\" \n",
    "        Calculates the mean length of noun phrases and verb phrases for all stories\n",
    "        in the corpus\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated \n",
    "            by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for mean length of noun and verb phrases\n",
    "    \"\"\"\n",
    "\n",
    "    noun_phrases, verb_phrases = phrase_parser(input_lists)\n",
    "\n",
    "    noun_phrase_lengths = phrase_length(noun_phrases)\n",
    "    verb_phrase_length = phrase_length(verb_phrases)\n",
    "    \n",
    "    feature_df['mean_np_length'] = noun_phrase_lengths\n",
    "    feature_df['mean_vp_length'] = verb_phrase_length\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the constituent counter and mean phrase length functions to the transcripts with constituency parsing completed\n",
    "linguistic_features = constituent_counter(parsed_transcripts, linguistic_features)\n",
    "linguistic_features = phrase_mean_length(parsed_transcripts, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the linguistic feature DataFrame\n",
    "linguistic_features.to_csv('linguistic_features/linguistic_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_descriptions(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\" \n",
    "        Calculates a variety of linguistic features (mean dependency distance, mean proportion of adjacent\n",
    "        dependency relations, first-order coherence, and second-order coherence) using the textdescription\n",
    "        library\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for each derived feature\n",
    "    \"\"\"\n",
    "     \n",
    "    text_description = pd.DataFrame()\n",
    "\n",
    "    for i in input_df.index:  \n",
    "        string = input_df['Text'].loc[i]\n",
    "        text_description_df = td.extract_metrics(text = string, metrics = [\"readability\", \"dependency_distance\", \"coherence\"], spacy_model = 'en_core_web_sm')\n",
    "        text_description = pd.concat([text_description_df, text_description])\n",
    "\n",
    "    text_description_ordered = (text_description[::-1]).reset_index()\n",
    "    text_description_ordered = text_description_ordered[['text', 'dependency_distance_mean', 'prop_adjacent_dependency_relation_mean', 'first_order_coherence', 'second_order_coherence']]\n",
    "    \n",
    "    return text_description_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the text description feature function to the transcripts with punctuation, and creating a separate DataFrame to store results\n",
    "text_description_ordered = text_descriptions(transcripts_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the text description DataFrame\n",
    "text_description_ordered.to_csv('linguistic_features/text_description_ordered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
