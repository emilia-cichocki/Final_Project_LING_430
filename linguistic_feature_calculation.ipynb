{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be skipped if the needed libraries are already installed\n",
    "# !pip install transformers\n",
    "# !pip install minicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"NotOpenSSLWarning\")\n",
    "warnings.filterwarnings(\"ignore\", message = r\"\\[W007\\].*\", category = UserWarning)\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from minicons import scorer\n",
    "import csv\n",
    "import textdescriptives as td\n",
    "import contractions\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "This section loads the narrative data necessary for the remainder of the script, including the text transcripts in a variety of formats (cleaned with punctuation, cleaned without punctuation, or separated by utterance). Additionally, it loads the language models and datasets for lexical feature analysis (SpaCy, word2vec, and minicons GPT2 surprisal scorer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the processed transcripts (cleaned and formatted, but retains original punctuation for SpaCy tokenization)\n",
    "spacy_transcripts = pd.read_csv('processed_data/transcripts_spacy_formatted.csv')\n",
    "\n",
    "# Creating a linguistic feature DataFrame that includes coherence scores\n",
    "linguistic_features = spacy_transcripts[['Coherence']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the transcripts with no punctuation\n",
    "transcripts_no_punc = pd.read_csv('processed_data/transcripts_no_punc_formatted.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the utterance-based transcriptions into a list\n",
    "story_lists = []\n",
    "with open('processed_data/sentences.csv', 'r', newline='') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        story_lists.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the SpaCy language processing model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word2vec model\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "sem_sim_model = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model for surprisal\n",
    "surprisal_model = scorer.IncrementalLMScorer('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5757ab60f2c4e168a029dbdb484e1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:34:15 INFO: Downloaded file to /Users/emicatx/stanza_resources/resources.json\n",
      "2025-12-03 13:34:15 INFO: Downloading default packages for language: en (English) ...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-12-03 13:34:17 INFO: File exists: /Users/emicatx/stanza_resources/en/default.zip\n",
      "2025-12-03 13:34:18 INFO: Finished downloading models and saved to /Users/emicatx/stanza_resources\n",
      "2025-12-03 13:34:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212ca0e62275450da30c10611462eb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:34:18 INFO: Downloaded file to /Users/emicatx/stanza_resources/resources.json\n",
      "2025-12-03 13:34:18 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-03 13:34:19 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2025-12-03 13:34:19 INFO: Using device: cpu\n",
      "2025-12-03 13:34:19 INFO: Loading: tokenize\n",
      "2025-12-03 13:34:19 INFO: Loading: mwt\n",
      "2025-12-03 13:34:19 INFO: Loading: pos\n",
      "2025-12-03 13:34:20 INFO: Loading: constituency\n",
      "2025-12-03 13:34:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Loading the stanza model\n",
    "stanza.download('en')\n",
    "stanza_model = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Calculations\n",
    "This section contains functions and code to calculate a series of linguistic features, particularly lexical and macrolinguistic metrics. It includes the following features (see [this document](README.md) for a more comprehensive description):\n",
    "- *Part-of-Speech Proportions*: number of words tagged as a particular part of speech (adjectives, adpositions, adverbs, auxiliaries, coordinating and subordinating conjunctions, determiners, interjections, nouns, numerals, particles, pronouns, proper nouns, verbs, punctuation, spaces, symbols, and unidentified characters) divided by total number of words\n",
    "- *Open-Closed Class Ratio*: ratio of number of open-class words (nouns, verbs, adjectives, and adverbs) to number of closed-class words (adpositions, auxiliaries, coordinating and subordinating conjunctions, determiners, interjections, numerals, particles, and pronouns)\n",
    "- *Propositional Density*: number of words introducing or carrying ideas (adjectives, adverbs, adpositions, coordinating and subordinating conjunctions, verbs) divided by total number of words\n",
    "- *Number of Words*: total number of words in a transcript\n",
    "- *Number of Utterances*: total number of utterances in a transcript\n",
    "- *Mean Words per Utterance*: mean number of words per utterance in a transcript\n",
    "- *Number of Logical Operators*: total number of logical operators (and, or, not, if...then) in a transcript\n",
    "- *Type-Token Ratio*: number of unique words divided by total number of words\n",
    "- *Semantic Diversity*: ***finish this***\n",
    "- *Age of Acquisition*: approximate age at which a word is learned, averaged across all words in a transcript with provided ratings\n",
    "- *Concreteness*: the extent to which the concept a word denotes refers to a perceptible entity ***citation!!!***, averaged across all words in a transcript with provided ratings\n",
    "- *Semantic Thematic Distance*: ***finish this***\n",
    "- *Surprisal*: degree to which a word is unexpected based on prior words, averaged across all words in a transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech_features(input_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the proportion of part-of-speech tags, open-closed class ratio, and propositional density\n",
    "        in a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for part-of-speech proportions, open-closed class ratio, and \n",
    "            propositional density\n",
    "    \"\"\"\n",
    "\n",
    "    pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB', 'X']\n",
    "    open_class_tags = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "    closed_class_tags = ['ADP', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NUM', 'PART', 'PRON', 'SCONJ']\n",
    "    propositional_density_tags = ['ADJ', 'ADV', 'ADP', 'CCONJ', 'SCONJ', 'VERB']\n",
    "\n",
    "    for i in feature_df.index:  \n",
    "\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        doc = nlp(string)\n",
    "        testing = []\n",
    "        open_class = 0\n",
    "        closed_class = 0\n",
    "        prop_density = 0\n",
    "\n",
    "        for token in doc:\n",
    "            testing.append(token.pos_)\n",
    "            if token.pos_ in open_class_tags:\n",
    "                open_class += 1\n",
    "            elif token.pos_ in closed_class_tags:\n",
    "                closed_class += 1\n",
    "            if token.pos_ in propositional_density_tags:\n",
    "                prop_density += 1\n",
    "        \n",
    "        for _, pos in enumerate(pos_tags):\n",
    "            try:\n",
    "                feature_df.loc[i, f'POS:{pos}'] = testing.count(pos)/len(testing) # TODO: make sure this covers everything, and add checks\n",
    "            except ZeroDivisionError:\n",
    "                print(\"No tokens have been identified in the input\")\n",
    "\n",
    "        try:\n",
    "            feature_df.loc[i, 'open_closed_ratio'] = open_class / closed_class\n",
    "        except ZeroDivisionError:\n",
    "            print(f\"Index {i} has no closed class words; check input\")\n",
    "\n",
    "        feature_df.loc[i, 'propositional_density'] = prop_density / len(testing)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the part-of-speech feature function to the SpaCy-formatted transcripts (cleaned with punctuation)\n",
    "linguistic_features = part_of_speech_features(spacy_transcripts ,linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utterance_number_features(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the number of utterances and the mean number of words per utterance in a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for number of utterances and mean number of words per utterance\n",
    "    \"\"\"\n",
    "\n",
    "    for i, story in enumerate(input_lists):\n",
    "        \n",
    "        feature_df.loc[i, 'n_utterances'] = len(story)\n",
    "\n",
    "        sentence_lengths = []\n",
    "        for sentence in story:\n",
    "            replaced_sentence = re.sub(r'[^A-Za-z0-9\\s]', '', sentence)\n",
    "            replaced_sentence = re.sub(r' +|\\n', ' ', replaced_sentence)\n",
    "            split_sentence = replaced_sentence.split(' ')\n",
    "            split_sentence = [word for word in split_sentence if word != '']\n",
    "            length = len(split_sentence)\n",
    "            sentence_lengths.append(length)\n",
    "\n",
    "        sentence_array = np.array(sentence_lengths)\n",
    "        mean = np.nanmean(sentence_array)\n",
    "        feature_df.loc[i, 'n_words_per_utterance'] = mean\n",
    "\n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coherence</th>\n",
       "      <th>POS:ADJ</th>\n",
       "      <th>POS:ADP</th>\n",
       "      <th>POS:ADV</th>\n",
       "      <th>POS:AUX</th>\n",
       "      <th>POS:CCONJ</th>\n",
       "      <th>POS:DET</th>\n",
       "      <th>POS:INTJ</th>\n",
       "      <th>POS:NOUN</th>\n",
       "      <th>POS:NUM</th>\n",
       "      <th>...</th>\n",
       "      <th>POS:PUNCT</th>\n",
       "      <th>POS:SCONJ</th>\n",
       "      <th>POS:SPACE</th>\n",
       "      <th>POS:SYM</th>\n",
       "      <th>POS:VERB</th>\n",
       "      <th>POS:X</th>\n",
       "      <th>open_closed_ratio</th>\n",
       "      <th>propositional_density</th>\n",
       "      <th>n_utterances</th>\n",
       "      <th>n_words_per_utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.044534</td>\n",
       "      <td>0.064777</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.068826</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.093117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125506</td>\n",
       "      <td>0.024291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.356275</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>0.059603</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109272</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.380795</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.091892</td>\n",
       "      <td>0.055135</td>\n",
       "      <td>0.060541</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.077838</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.139459</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152432</td>\n",
       "      <td>0.028108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.879795</td>\n",
       "      <td>0.375135</td>\n",
       "      <td>82.0</td>\n",
       "      <td>9.243902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.071806</td>\n",
       "      <td>0.080253</td>\n",
       "      <td>0.069694</td>\n",
       "      <td>0.055966</td>\n",
       "      <td>0.030623</td>\n",
       "      <td>0.080253</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.165787</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.036082</td>\n",
       "      <td>0.376980</td>\n",
       "      <td>86.0</td>\n",
       "      <td>9.441860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.061111</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165254</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.067961</td>\n",
       "      <td>0.453390</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>4</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>3</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.061224</td>\n",
       "      <td>0.419643</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>5</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138158</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>5</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coherence   POS:ADJ   POS:ADP   POS:ADV   POS:AUX  POS:CCONJ   POS:DET  \\\n",
       "0             4  0.044534  0.064777  0.076923  0.068826   0.020243  0.093117   \n",
       "1             5  0.043046  0.076159  0.076159  0.059603   0.026490  0.076159   \n",
       "2             5  0.054054  0.091892  0.055135  0.060541   0.022703  0.077838   \n",
       "3             4  0.071806  0.080253  0.069694  0.055966   0.030623  0.080253   \n",
       "4             5  0.033333  0.038889  0.033333  0.061111   0.044444  0.094444   \n",
       "...         ...       ...       ...       ...       ...        ...       ...   \n",
       "1534          3  0.084746  0.059322  0.072034  0.067797   0.055085  0.042373   \n",
       "1535          4  0.074074  0.055556  0.129630  0.074074   0.046296  0.055556   \n",
       "1536          3  0.071429  0.098214  0.044643  0.044643   0.035714  0.107143   \n",
       "1537          5  0.032895  0.065789  0.052632  0.072368   0.032895  0.078947   \n",
       "1538          5  0.081633  0.081633  0.010204  0.071429   0.010204  0.163265   \n",
       "\n",
       "      POS:INTJ  POS:NOUN   POS:NUM  ...  POS:PUNCT  POS:SCONJ  POS:SPACE  \\\n",
       "0     0.000000  0.141700  0.008097  ...   0.125506   0.024291        0.0   \n",
       "1     0.000000  0.109272  0.009934  ...   0.125828   0.016556        0.0   \n",
       "2     0.002162  0.139459  0.009730  ...   0.152432   0.028108        0.0   \n",
       "3     0.001056  0.165787  0.013728  ...   0.141499   0.007392        0.0   \n",
       "4     0.000000  0.177778  0.016667  ...   0.138889   0.033333        0.0   \n",
       "...        ...       ...       ...  ...        ...        ...        ...   \n",
       "1534  0.000000  0.165254  0.004237  ...   0.097458   0.038136        0.0   \n",
       "1535  0.000000  0.111111  0.000000  ...   0.092593   0.037037        0.0   \n",
       "1536  0.000000  0.214286  0.008929  ...   0.098214   0.035714        0.0   \n",
       "1537  0.000000  0.138158  0.013158  ...   0.118421   0.019737        0.0   \n",
       "1538  0.000000  0.234694  0.000000  ...   0.081633   0.030612        0.0   \n",
       "\n",
       "      POS:SYM  POS:VERB  POS:X  open_closed_ratio  propositional_density  \\\n",
       "0         0.0  0.125506    0.0           0.820513               0.356275   \n",
       "1         0.0  0.142384    0.0           0.788732               0.380795   \n",
       "2         0.0  0.123243    0.0           0.879795               0.375135   \n",
       "3         0.0  0.117212    0.0           1.036082               0.376980   \n",
       "4         0.0  0.155556    0.0           0.878049               0.338889   \n",
       "...       ...       ...    ...                ...                    ...   \n",
       "1534      0.0  0.144068    0.0           1.067961               0.453390   \n",
       "1535      0.0  0.138889    0.0           1.000000               0.481481   \n",
       "1536      0.0  0.133929    0.0           1.061224               0.419643   \n",
       "1537      0.0  0.164474    0.0           0.830986               0.368421   \n",
       "1538      0.0  0.112245    0.0           1.023810               0.326531   \n",
       "\n",
       "      n_utterances  n_words_per_utterance  \n",
       "0             21.0              10.000000  \n",
       "1             21.0              12.190476  \n",
       "2             82.0               9.243902  \n",
       "3             86.0               9.441860  \n",
       "4             11.0              14.090909  \n",
       "...            ...                    ...  \n",
       "1534          14.0              14.714286  \n",
       "1535           6.0              16.000000  \n",
       "1536           7.0              14.428571  \n",
       "1537          11.0              11.909091  \n",
       "1538           5.0              18.000000  \n",
       "\n",
       "[1539 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linguistic_features = utterance_number_features(story_lists, linguistic_features)\n",
    "display(linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_operators(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\" \n",
    "        Calculates the sum of logical operators (and, or, not, if...then) in a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for the number of logical operators\n",
    "    \"\"\"\n",
    "\n",
    "    logical_operator_list = [\n",
    "        (r'^And'),\n",
    "        (r' and[!?\\-,.:;\\'\"\\s]'),\n",
    "        (r' not[!?\\-,.:;\\'\"\\s]'),\n",
    "        (r' or[!?\\-,.:;\\'\"\\s]'),\n",
    "        (r' (if) (.*?)(then)[!?\\-…,.:;\\'\"\\s]')\n",
    "    ]\n",
    "\n",
    "    for i, story in enumerate(input_lists):\n",
    "        logical_operator_number = []\n",
    "        for string in story:\n",
    "            for pattern in logical_operator_list:\n",
    "                match = re.findall(pattern, string, flags = re.IGNORECASE)\n",
    "                n_operators = len(match)\n",
    "                logical_operator_number.append(n_operators)\n",
    "        feature_df.loc[i, 'n_logical_operators'] = np.sum(logical_operator_number)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = logical_operators(story_lists, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_ttr(string: str) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the type-token ratio (number of unique words divided by total number of words)\n",
    "        of a string\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        string: str\n",
    "            String containing text without punctuation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[float, float]\n",
    "            Value of type-token ratio and number of tokens in the string\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError \n",
    "            If type-token ratio is greater than 1\n",
    "        ValueError\n",
    "            If type-token ratio is negative\n",
    "    \"\"\"\n",
    "\n",
    "    string_lower = string.lower()\n",
    "    string_split = string_lower.split(' ')\n",
    "    string_set = set(string_split)\n",
    "    n_tokens = len(string_split)\n",
    "    types = len(string_set)\n",
    "\n",
    "    try:\n",
    "        ttr = types / n_tokens\n",
    "    except ZeroDivisionError:\n",
    "        print(f\"Input string has no tokens (empty string)\")\n",
    "\n",
    "    if ttr > 1:\n",
    "        raise ValueError('Type-token ratio cannot exceed 1')\n",
    "    if ttr < 0:\n",
    "        raise ValueError('Type-token ratio cannot be negative')\n",
    "\n",
    "    return ttr, n_tokens\n",
    "\n",
    "\n",
    "def word_count_features(input_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates the number of words and type-token ratio for each transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for type-token ratio and total number of words\n",
    "    \"\"\"\n",
    "\n",
    "    for i in input_df.index:\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        feature_df.loc[i, 'type_token_ratio'], feature_df.loc[i, 'n_words'] = string_ttr(string)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = word_count_features(transcripts_no_punc, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_features(dictionary: dict, feature: str, feature_df: pd.DataFrame, input_df: pd.DataFrame = transcripts_no_punc) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Base function for calculating a series of lexical features (used for concreteness, age of \n",
    "        acquisition, word frequency, and semantic diversity)\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        dictionary: dict:\n",
    "            Dictionary containing words and associated pre-defined values, imported from\n",
    "            external sources\n",
    "        feature: str\n",
    "            Feature used to name column of feature DataFrame\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text', defaults to transcripts\n",
    "            with no punctuation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with columns for the relevant feature\n",
    "    \"\"\"\n",
    "\n",
    "    for i in input_df.index:\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        string_split = (string.lower()).split(' ')\n",
    "        values = []\n",
    "\n",
    "        for item in string_split:\n",
    "            value = dictionary.get(item)\n",
    "\n",
    "            if value is not None:\n",
    "                values.append(value)\n",
    "        mean = np.nanmean(np.array(values))\n",
    "\n",
    "        feature_df.loc[i, feature] = mean\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Frequency (logarithm)\n",
    "\n",
    "frequency = pd.read_csv('lexical_feature_data/SUBTLEXusExcel2007.csv')\n",
    "frequency_dict_original = dict(zip(frequency['Word'], frequency['Lg10WF']))\n",
    "frequency_dict = {(word.lower() if isinstance(word, str) else word): value for word, value in frequency_dict_original.items()}\n",
    "\n",
    "linguistic_features = lexical_features(frequency_dict, 'log10_freq_mean', linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Diversity\n",
    "\n",
    "semantic_diversity = pd.read_csv('lexical_feature_data/13428_2012_278_MOESM1_ESM.csv', dtype = str)\n",
    "semantic_diversity = (semantic_diversity.rename(columns = {'Supplementary Materials: SemD values': 'word', 'Unnamed: 2': 'sem_diversity'})).drop(0)\n",
    "semantic_diversity_dict_original = dict(zip(semantic_diversity['word'], semantic_diversity['sem_diversity']))\n",
    "semantic_diversity_dict = {(word.lower() if isinstance(word, str) else word): (float(value)) for word, value in semantic_diversity_dict_original.items()}\n",
    "\n",
    "\n",
    "linguistic_features = lexical_features(semantic_diversity_dict, 'semantic_diversity_mean', linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age of Acquisition\n",
    "\n",
    "aoa = pd.read_csv('lexical_feature_data/AoA_ratings_Kuperman_et_al_BRM.txt', sep = '\\t')\n",
    "aoa_dict_original = dict(zip(aoa['Word'], aoa['Rating.Mean']))\n",
    "aoa_dict = {(word.lower() if isinstance(word, str) else word): (float(value)) for word, value in aoa_dict_original.items()}\n",
    "\n",
    "linguistic_features = lexical_features(aoa_dict, 'aoa_mean', linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concreteness\n",
    "\n",
    "concreteness = pd.read_csv('lexical_feature_data/concreteness_ratings.csv')\n",
    "concreteness_dict_original = dict(zip(concreteness['Word'], concreteness['Conc.M']))\n",
    "concreteness_dict = {(word.lower() if isinstance(word, str) else word): (float(value)) for word, value in concreteness_dict_original.items()}\n",
    "\n",
    "linguistic_features = lexical_features(concreteness_dict, 'concreteness_mean', linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_thematic_distance(input_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates semantic thematic distance (cosine similarity between each adjacent word pair\n",
    "        in a transcript)\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_df: pd.DataFrame\n",
    "            Input DataFrame with column storing transcripts as 'Text'\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for semantic thematic distance\n",
    "        \"\"\"\n",
    "   \n",
    "    for i in input_df.index:  \n",
    "\n",
    "        string = input_df['Text'].iloc[i]\n",
    "        string_split = (string.lower()).split(' ')\n",
    "        sem_sim = []\n",
    "\n",
    "        for j in range(len(string_split) - 1):\n",
    "            try:\n",
    "                sem_sim_value = sem_sim_model.similarity(string_split[j], string_split[j + 1])\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "            sem_sim.append(float(sem_sim_value))\n",
    "        \n",
    "        sem_thematic_distance = np.nanmean(np.array(sem_sim))\n",
    "\n",
    "        feature_df.loc[i, 'semantic_thematic_distance'] = sem_thematic_distance\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = semantic_thematic_distance(transcripts_no_punc, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprisal(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Calculates average surprisal for a transcript\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        input_lists: list\n",
    "            Input lists with each sub-list containing story transcripts separated by utterance\n",
    "        feature_df: pd.DataFrame\n",
    "            DataFrame storing coherence scores and linguistic features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Feature DataFrame with column for surprisal\n",
    "        \"\"\"\n",
    "    \n",
    "    for i, story in enumerate(input_lists):\n",
    "        transcript_surprisal = []\n",
    "        \n",
    "        for sentence in story:\n",
    "            sentence_surprisal = surprisal_model.sequence_score(sentence, reduction = lambda x: -x.mean(0).item())\n",
    "            transcript_surprisal.append(sentence_surprisal)\n",
    "        \n",
    "        transcript_surprisal_array = np.array(transcript_surprisal)\n",
    "        surprisal_mean = np.nanmean(transcript_surprisal_array)\n",
    "\n",
    "        feature_df.loc[i, 'surprisal'] = surprisal_mean\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = surprisal(story_lists, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coherence</th>\n",
       "      <th>POS:ADJ</th>\n",
       "      <th>POS:ADP</th>\n",
       "      <th>POS:ADV</th>\n",
       "      <th>POS:AUX</th>\n",
       "      <th>POS:CCONJ</th>\n",
       "      <th>POS:DET</th>\n",
       "      <th>POS:INTJ</th>\n",
       "      <th>POS:NOUN</th>\n",
       "      <th>POS:NUM</th>\n",
       "      <th>...</th>\n",
       "      <th>n_words_per_utterance</th>\n",
       "      <th>n_logical_operators</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>n_words</th>\n",
       "      <th>log10_freq_mean</th>\n",
       "      <th>semantic_diversity_mean</th>\n",
       "      <th>aoa_mean</th>\n",
       "      <th>concreteness_mean</th>\n",
       "      <th>semantic_thematic_distance</th>\n",
       "      <th>surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.044534</td>\n",
       "      <td>0.064777</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.068826</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.093117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4.682792</td>\n",
       "      <td>2.107979</td>\n",
       "      <td>4.616221</td>\n",
       "      <td>2.597958</td>\n",
       "      <td>0.198185</td>\n",
       "      <td>4.895232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>0.059603</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109272</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>...</td>\n",
       "      <td>12.190476</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4.666692</td>\n",
       "      <td>2.069585</td>\n",
       "      <td>4.606250</td>\n",
       "      <td>2.615833</td>\n",
       "      <td>0.192592</td>\n",
       "      <td>4.183853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.091892</td>\n",
       "      <td>0.055135</td>\n",
       "      <td>0.060541</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.077838</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.139459</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>...</td>\n",
       "      <td>9.243902</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.509235</td>\n",
       "      <td>758.0</td>\n",
       "      <td>4.396347</td>\n",
       "      <td>2.102881</td>\n",
       "      <td>5.347357</td>\n",
       "      <td>2.420347</td>\n",
       "      <td>0.180181</td>\n",
       "      <td>5.146196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.071806</td>\n",
       "      <td>0.080253</td>\n",
       "      <td>0.069694</td>\n",
       "      <td>0.055966</td>\n",
       "      <td>0.030623</td>\n",
       "      <td>0.080253</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.165787</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>...</td>\n",
       "      <td>9.441860</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.492611</td>\n",
       "      <td>812.0</td>\n",
       "      <td>4.421817</td>\n",
       "      <td>2.076899</td>\n",
       "      <td>5.135905</td>\n",
       "      <td>2.673849</td>\n",
       "      <td>0.176816</td>\n",
       "      <td>5.596125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.061111</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>...</td>\n",
       "      <td>14.090909</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.593548</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.596490</td>\n",
       "      <td>2.064892</td>\n",
       "      <td>5.063814</td>\n",
       "      <td>2.591143</td>\n",
       "      <td>0.161092</td>\n",
       "      <td>4.702588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165254</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>206.0</td>\n",
       "      <td>4.404882</td>\n",
       "      <td>2.118607</td>\n",
       "      <td>5.127013</td>\n",
       "      <td>2.415114</td>\n",
       "      <td>0.207649</td>\n",
       "      <td>3.565189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>4</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.578900</td>\n",
       "      <td>2.106413</td>\n",
       "      <td>5.049481</td>\n",
       "      <td>2.350449</td>\n",
       "      <td>0.222453</td>\n",
       "      <td>3.443641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>3</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>...</td>\n",
       "      <td>14.428571</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.801980</td>\n",
       "      <td>101.0</td>\n",
       "      <td>4.368046</td>\n",
       "      <td>2.086000</td>\n",
       "      <td>5.140270</td>\n",
       "      <td>2.389186</td>\n",
       "      <td>0.172722</td>\n",
       "      <td>4.055240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>5</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138158</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>...</td>\n",
       "      <td>11.909091</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.633588</td>\n",
       "      <td>131.0</td>\n",
       "      <td>4.474590</td>\n",
       "      <td>2.176371</td>\n",
       "      <td>5.095055</td>\n",
       "      <td>2.484435</td>\n",
       "      <td>0.196065</td>\n",
       "      <td>3.690323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>5</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>90.0</td>\n",
       "      <td>4.317540</td>\n",
       "      <td>2.041781</td>\n",
       "      <td>5.834627</td>\n",
       "      <td>2.441184</td>\n",
       "      <td>0.156048</td>\n",
       "      <td>4.243364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coherence   POS:ADJ   POS:ADP   POS:ADV   POS:AUX  POS:CCONJ   POS:DET  \\\n",
       "0             4  0.044534  0.064777  0.076923  0.068826   0.020243  0.093117   \n",
       "1             5  0.043046  0.076159  0.076159  0.059603   0.026490  0.076159   \n",
       "2             5  0.054054  0.091892  0.055135  0.060541   0.022703  0.077838   \n",
       "3             4  0.071806  0.080253  0.069694  0.055966   0.030623  0.080253   \n",
       "4             5  0.033333  0.038889  0.033333  0.061111   0.044444  0.094444   \n",
       "...         ...       ...       ...       ...       ...        ...       ...   \n",
       "1534          3  0.084746  0.059322  0.072034  0.067797   0.055085  0.042373   \n",
       "1535          4  0.074074  0.055556  0.129630  0.074074   0.046296  0.055556   \n",
       "1536          3  0.071429  0.098214  0.044643  0.044643   0.035714  0.107143   \n",
       "1537          5  0.032895  0.065789  0.052632  0.072368   0.032895  0.078947   \n",
       "1538          5  0.081633  0.081633  0.010204  0.071429   0.010204  0.163265   \n",
       "\n",
       "      POS:INTJ  POS:NOUN   POS:NUM  ...  n_words_per_utterance  \\\n",
       "0     0.000000  0.141700  0.008097  ...              10.000000   \n",
       "1     0.000000  0.109272  0.009934  ...              12.190476   \n",
       "2     0.002162  0.139459  0.009730  ...               9.243902   \n",
       "3     0.001056  0.165787  0.013728  ...               9.441860   \n",
       "4     0.000000  0.177778  0.016667  ...              14.090909   \n",
       "...        ...       ...       ...  ...                    ...   \n",
       "1534  0.000000  0.165254  0.004237  ...              14.714286   \n",
       "1535  0.000000  0.111111  0.000000  ...              16.000000   \n",
       "1536  0.000000  0.214286  0.008929  ...              14.428571   \n",
       "1537  0.000000  0.138158  0.013158  ...              11.909091   \n",
       "1538  0.000000  0.234694  0.000000  ...              18.000000   \n",
       "\n",
       "      n_logical_operators  type_token_ratio  n_words  log10_freq_mean  \\\n",
       "0                     4.0          0.609524    210.0         4.682792   \n",
       "1                     7.0          0.535156    256.0         4.666692   \n",
       "2                    18.0          0.509235    758.0         4.396347   \n",
       "3                    30.0          0.492611    812.0         4.421817   \n",
       "4                     9.0          0.593548    155.0         4.596490   \n",
       "...                   ...               ...      ...              ...   \n",
       "1534                 10.0          0.669903    206.0         4.404882   \n",
       "1535                  3.0          0.791667     96.0         4.578900   \n",
       "1536                  4.0          0.801980    101.0         4.368046   \n",
       "1537                  3.0          0.633588    131.0         4.474590   \n",
       "1538                  1.0          0.666667     90.0         4.317540   \n",
       "\n",
       "      semantic_diversity_mean  aoa_mean  concreteness_mean  \\\n",
       "0                    2.107979  4.616221           2.597958   \n",
       "1                    2.069585  4.606250           2.615833   \n",
       "2                    2.102881  5.347357           2.420347   \n",
       "3                    2.076899  5.135905           2.673849   \n",
       "4                    2.064892  5.063814           2.591143   \n",
       "...                       ...       ...                ...   \n",
       "1534                 2.118607  5.127013           2.415114   \n",
       "1535                 2.106413  5.049481           2.350449   \n",
       "1536                 2.086000  5.140270           2.389186   \n",
       "1537                 2.176371  5.095055           2.484435   \n",
       "1538                 2.041781  5.834627           2.441184   \n",
       "\n",
       "      semantic_thematic_distance  surprisal  \n",
       "0                       0.198185   4.895232  \n",
       "1                       0.192592   4.183853  \n",
       "2                       0.180181   5.146196  \n",
       "3                       0.176816   5.596125  \n",
       "4                       0.161092   4.702588  \n",
       "...                          ...        ...  \n",
       "1534                    0.207649   3.565189  \n",
       "1535                    0.222453   3.443641  \n",
       "1536                    0.172722   4.055240  \n",
       "1537                    0.196065   3.690323  \n",
       "1538                    0.156048   4.243364  \n",
       "\n",
       "[1539 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(linguistic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***TODO: SEGMENT OUT THESE FEATURE TYPES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_cohesion_arrays(input_lists: list) -> np.ndarray:\n",
    "    \n",
    "    # TODO: type hints, docstrings\n",
    "    noun_pos = ['NOUN', 'PROPN', 'PRON']\n",
    "\n",
    "    story_lists_updated = []\n",
    "    for story in input_lists:\n",
    "        story_nouns = []\n",
    "        for sentence in story:\n",
    "            sentence_nouns = []\n",
    "            sentence_str = str(sentence)\n",
    "            doc = nlp(contractions.fix(sentence_str.lower()))\n",
    "            for token in doc:\n",
    "                if token.pos_ in noun_pos:\n",
    "                    sentence_nouns.append(str(token))\n",
    "            story_nouns.append(sentence_nouns)\n",
    "        story_lists_updated.append(story_nouns)\n",
    "\n",
    "        \n",
    "    all_cohesion_arrays = np.ndarray((len(input_lists),), dtype = np.ndarray)\n",
    "\n",
    "    for k in range(len(story_lists_updated)):\n",
    "        string = story_lists_updated[k]\n",
    "        cohesion_array = np.zeros((len(string), len(string)))\n",
    "        for i in range(len(string)):\n",
    "            current_string = string[i]\n",
    "            for j in range(len(string)):\n",
    "                new_string = string[j]\n",
    "                if any(item in new_string for item in current_string):\n",
    "                    cohesion_array[i, j] = 1\n",
    "        all_cohesion_arrays[k] = cohesion_array\n",
    "\n",
    "    for array in all_cohesion_arrays: # TODO: describe check for symmetry\n",
    "        if not np.array_equal(array, array.T):\n",
    "            raise ValueError(\"Array must be symmetric; check input\")\n",
    "\n",
    "    return all_cohesion_arrays\n",
    "\n",
    "def local_global_cohesion(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # TODO: type hints, docstrings\n",
    "\n",
    "    input_array = noun_cohesion_arrays(input_lists)\n",
    "\n",
    "    for n, array in enumerate(input_array):\n",
    "        counter = 0\n",
    "        for i in range(array.shape[0] - 1):\n",
    "                counter += array[i, i+1]\n",
    "        try:\n",
    "            coref_local = counter / (array.shape[0] - 1)\n",
    "        except ZeroDivisionError:\n",
    "            coref_local = 0 \n",
    "\n",
    "        feature_df.loc[n, 'coref_local'] = coref_local\n",
    "\n",
    "    feature_df = feature_df.sort_index()\n",
    "\n",
    "    np.seterr('raise')\n",
    "\n",
    "    for i, array in enumerate(input_array):\n",
    "        n = array.shape[0]\n",
    "        counter = (array.sum() - np.diag(array).sum()) / 2\n",
    "        try:\n",
    "            coref_global = counter / (n * (n - 1) / 2)\n",
    "        except Exception as e:\n",
    "            coref_global = 0 \n",
    "\n",
    "        feature_df.loc[i, 'coref_global'] = coref_global\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = local_global_cohesion(story_lists, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation\n",
    "\n",
    "def parser(input_df: pd.DataFrame) -> list:\n",
    "\n",
    "    parsed_transcripts = []\n",
    "\n",
    "    for i in input_df.index:\n",
    "        string = input_df['Text'].loc[i]\n",
    "        doc = stanza_model(string)\n",
    "        story_updated = []\n",
    "        for sentence in doc.sentences:\n",
    "            tree = sentence.constituency\n",
    "            string_tree = str(tree).split(' ')\n",
    "            story_updated.append(string_tree)\n",
    "\n",
    "        parsed_transcripts.append(story_updated)\n",
    "    \n",
    "    return parsed_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_transcripts = parser(spacy_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituent_counter(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # TODO: type hints, docstrings\n",
    "\n",
    "    counter_pos = [\"(NP\", \"(VP\", \"(SBAR\", \"(PP\"]\n",
    "\n",
    "    for i, story in enumerate(input_lists):\n",
    "        counter = 0\n",
    "        type_counter = [0, 0, 0, 0]\n",
    "        for sentence in story:\n",
    "            for item in sentence:\n",
    "                if item in counter_pos:\n",
    "                    counter += 1\n",
    "                    idx = counter_pos.index(item)\n",
    "                    type_counter[idx] += 1\n",
    "\n",
    "        feature_df.loc[i, 'n_constituents'] = counter\n",
    "        feature_df.loc[i, 'n_noun_phrases'] = type_counter[0]\n",
    "        feature_df.loc[i, 'n_verb_phrases'] = type_counter[1]\n",
    "        feature_df.loc[i, 'n_sub_clauses'] = type_counter[2]\n",
    "        feature_df.loc[i, 'n_prep_phrases'] = type_counter[3]\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "def phrase_parser(input_lists: list) -> Tuple[list, list]:\n",
    "    \n",
    "    \n",
    "    # TODO: Noun and verb phrase parser\n",
    "\n",
    "    noun_phrases_corpus = []\n",
    "    verb_phrases_corpus = []\n",
    "    closing_pattern = r'\\)'\n",
    "\n",
    "    phrasal_pos = [\"(NP\", \"(VP\"]\n",
    "\n",
    "    for story in input_lists:\n",
    "        noun_phrases_overall = []\n",
    "        verb_phrases_overall = []\n",
    "        for sentence in story:\n",
    "            noun_phrases = []\n",
    "            verb_phrases = []\n",
    "            for i in range(len(sentence)):\n",
    "                item = sentence[i]\n",
    "                if item in phrasal_pos:\n",
    "                    open_counter = 1\n",
    "                    for j in range(i + 1, len(sentence)):\n",
    "                        first_element = sentence[j][0]\n",
    "                        match = re.findall(closing_pattern, sentence[j])\n",
    "                        closed = len(match)\n",
    "                        if first_element == '(':\n",
    "                            open_counter += 1\n",
    "                        open_counter -= closed\n",
    "                        if open_counter <= 0:\n",
    "                            break\n",
    "                    phrase = [item for item in sentence[i:j + 1] if item[-1] == ')']\n",
    "                    if item == phrasal_pos[0]:\n",
    "                        noun_phrases.append(phrase)\n",
    "                    else:\n",
    "                        verb_phrases.append(phrase)\n",
    "\n",
    "            noun_phrases_overall.append(noun_phrases)\n",
    "            verb_phrases_overall.append(verb_phrases)\n",
    "\n",
    "        noun_phrases_corpus.append(noun_phrases_overall)\n",
    "        verb_phrases_corpus.append(verb_phrases_overall)\n",
    "\n",
    "    return noun_phrases_corpus, verb_phrases_corpus\n",
    "\n",
    "def phrase_length(phrase_list: list) -> list:\n",
    "    \n",
    "    # TODO: type hints + docstrings\n",
    "\n",
    "    phrase_lengths = []\n",
    "\n",
    "    for story in phrase_list:\n",
    "        length_list = []\n",
    "        for sentence in story:\n",
    "            for phrase in sentence:\n",
    "                length = len(phrase)\n",
    "                length_list.append(length)\n",
    "\n",
    "        length_array = np.array(length_list)\n",
    "        mean = length_array.mean()\n",
    "        phrase_lengths.append(float(mean))\n",
    "\n",
    "    return phrase_lengths\n",
    "\n",
    "def phrase_mean_length(input_lists: list, feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # TODO: type hints + docstrings\n",
    "\n",
    "    noun_phrases, verb_phrases = phrase_parser(input_lists)\n",
    "\n",
    "    noun_phrase_lengths = phrase_length(noun_phrases)\n",
    "    verb_phrase_length = phrase_length(verb_phrases)\n",
    "    \n",
    "    feature_df['mean_np_length'] = noun_phrase_lengths\n",
    "    feature_df['mean_vp_length'] = verb_phrase_length\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = constituent_counter(parsed_transcripts, linguistic_features)\n",
    "linguistic_features = phrase_mean_length(parsed_transcripts, linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the linguistic feature DataFrame\n",
    "linguistic_features.to_csv('linguistic_features/linguistic_features.csv')\n",
    "# TODO: move anything that doesn't require the spacy ling features to another section (lexical, macrolinguistic, syntactic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_descriptions(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    text_description = pd.DataFrame()\n",
    "\n",
    "    for i in input_df.index:  \n",
    "        string = input_df['Text'].loc[i]\n",
    "        text_description_df = td.extract_metrics(text = string, metrics = [\"readability\", \"dependency_distance\", \"coherence\"], spacy_model = 'en_core_web_sm')\n",
    "        text_description = pd.concat([text_description_df, text_description])\n",
    "\n",
    "    text_description_ordered = (text_description[::-1]).reset_index()\n",
    "    text_description_ordered = text_description_ordered[['text', 'dependency_distance_mean', 'prop_adjacent_dependency_relation_mean', 'first_order_coherence', 'second_order_coherence']]\n",
    "    \n",
    "    return text_description_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_description_ordered = text_descriptions(spacy_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_description_ordered.to_csv('linguistic_features/text_description_ordered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
